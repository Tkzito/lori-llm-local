# Lori LLM Local

Lori √© uma assistente 100% local para linha de comando e navegador constru√≠da sobre modelos servidos pelo Ollama. A stack combina:

- **Backend FastAPI** com WebSocket para streaming do racioc√≠nio do agente.
- **Interface web reativa** (HTML + CSS + JS puro) com hist√≥rico agrupado, painel de racioc√≠nio e envio de arquivos de contexto.
- **CLI** integrada ao mesmo n√∫cleo (`assistant_cli`) e um conjunto modular de ferramentas (leitura/escrita de arquivos, Git, busca web, cota√ß√µes, etc.).

---

## üÜï Atualiza√ß√µes recentes

- Hist√≥rico reorganizado por dia (Hoje, Ontem e nome da semana), exibindo t√≠tulo, hor√°rio e pr√©via de cada conversa.
- Handles laterais (‚ò∞ √† esquerda e üß† √† direita) para recolher/reabrir hist√≥rico e painel de racioc√≠nio sem ocupar o espa√ßo do chat.
- README revisado para refletir o menu unificado, o fluxo atual da Web UI e o roteiro de evolu√ß√£o.

---

## üöÄ Primeiros passos

### Pr√©-requisitos

- Python 3.10 ou superior.
- Ollama instalado e em execu√ß√£o (`http://localhost:11434` por padr√£o).
- Recomendado baixar previamente um modelo, ex.: `ollama pull mistral`.

### Inicializa√ß√£o via menu unificado

```bash
./start.sh
```

O script oferece as op√ß√µes:

1. Iniciar Lori no terminal (CLI).
2. Iniciar Lori no navegador (Web UI).
3. Verificar/iniciar o Ollama local.
4. Encerrar o Ollama.
5. Iniciar tudo (Ollama + Web UI em segundo plano).
6. Encerrar tudo.
7. Visualizar logs (Ollama, Web UI ou ambos).

O menu:

- Garante a cria√ß√£o/uso do virtualenv `.venv` e instala `requirements.txt` quando necess√°rio.
- Mant√©m a Web UI dispon√≠vel em `http://127.0.0.1:8001/`.
- Registra PIDs (`.lori_web.pid`, `.lori_ollama.pid`) e logs (`.lori_web.log`, `.lori_ollama.log`) para f√°cil inspe√ß√£o.

### Ver logs rapidamente

```bash
# Log apenas do Ollama
./lori-logs.sh ollama

# Log da Web UI quando est√° em segundo plano
./lori-logs.sh web

# Ambos em paralelo
./lori-logs.sh ambos
```

### Execu√ß√£o direta (opcional)

```bash
./run.sh       # CLI
./run_web.sh   # Web UI
```

---

## üñ•Ô∏è Vis√£o geral da Web UI

A interface √© composta por tr√™s zonas:

| Zona | Descri√ß√£o |
| --- | --- |
| **Hist√≥rico** (esquerda) | Conversas agrupadas por dia, com t√≠tulo, pr√©via e hor√°rio. Pode ser ocultado pelo bot√£o ‚ò∞ e reaberto pelo handle lateral. |
| **Chat** (centro) | √Årea principal da conversa, com envio `Enter`, indicador de digita√ß√£o e suporte a anexos. |
| **Racioc√≠nio do agente** (direita) | Mostra pensamentos, chamadas de ferramentas e confirma√ß√µes. Pode ser recolhido pelo bot√£o üß† ou reaberto pelo handle com √≠cone de c√©rebro. |

### Anexando arquivos de contexto

- Bot√£o üìé adiciona arquivos; eles s√£o salvos em `~/lori/uploads`.
- Cada item exibe nome, tamanho e bot√£o **Remover** com feedback visual (spinner) durante opera√ß√µes.
- Bot√£o **Limpar** remove todos os anexos; o contador indica quantos arquivos est√£o ativos.

### Outros recursos

- Altern√¢ncia entre tema claro/escuro (√≠cone ‚óë).
- Hist√≥rico e painel de racioc√≠nio lembram o estado (aberto/recolhido) via `localStorage`.
- Handles laterais permitem abrir pain√©is rapidamente sem comprometer o espa√ßo do chat.

### Evolu√ß√µes planejadas

- Automa√ß√£o para detectar pedidos de atualiza√ß√£o (‚Äúatualize o pre√ßo do BTC‚Äù) e disparar consultas relevantes automaticamente.
- Heur√≠sticas que utilizam o hist√≥rico agrupado para inferir prefer√™ncias por conversa.

---

## üìÅ Estrutura do projeto

```
assistant-cli/
‚îú‚îÄ‚îÄ assistant_cli/          # N√∫cleo do agente e ferramentas
‚îÇ   ‚îú‚îÄ‚îÄ agent.py            # Loop principal do agente
‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # Entrada do CLI
‚îÇ   ‚îú‚îÄ‚îÄ tools.py            # Registro de ferramentas/bindings
‚îÇ   ‚îî‚îÄ‚îÄ config.py           # Diret√≥rios padr√£o e vari√°veis de ambiente
‚îú‚îÄ‚îÄ web/                    # Backend FastAPI (REST + WebSocket)
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Rotas/servi√ßos
‚îÇ   ‚îî‚îÄ‚îÄ static/             # Front-end (index.html, style.css, app.js)
‚îú‚îÄ‚îÄ run.sh                  # Inicializador do CLI
‚îú‚îÄ‚îÄ run_web.sh              # Inicializador da Web UI
‚îú‚îÄ‚îÄ start.sh                # Menu unificado com bootstrapping
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îî‚îÄ‚îÄ config.ini.template     # Template opcional
```

---

## ‚öôÔ∏è Configura√ß√£o

Principais vari√°veis em `assistant_cli/config.py`:

| Vari√°vel | Descri√ß√£o | Padr√£o |
| --- | --- | --- |
| `ASSISTANT_MODEL` | Modelo utilizado via Ollama | `mistral` |
| `OLLAMA_BASE_URL` | Endpoint do Ollama | `http://localhost:11434` |
| `LORI_HOME` | Diret√≥rio base (`workspace`, `uploads`, etc.) | `~/lori` |
| `ASSISTANT_ROOT` | Raiz permitida para opera√ß√µes de arquivo | `~/lori/workspace` |
| `ASSISTANT_VERBOSE` | Ativa logs detalhados de ferramentas | `0` |
| `OLLAMA_USE_GPU` | For√ßa uso de GPU (`1`) | auto |

> **Dica GPU:** instale a vers√£o do Ollama com suporte CUDA, exporte `OLLAMA_USE_GPU=1` (ou configure `~/.ollama/config`) e baixe o modelo desejado antes de iniciar o menu.

---

## üß™ Desenvolvimento e testes

- **Testes:** `pytest`
- **Lint:** `ruff check .`
- Altera√ß√µes em `web/static` s√£o aplicadas ao recarregar o navegador; o backend pode rodar com `uvicorn --reload`.
- Anexos ficam em `~/lori/uploads` ‚Äî limpe manualmente se precisar.

---

## üõ†Ô∏è Solu√ß√£o de problemas

| Sintoma | Como resolver |
| --- | --- |
| Modelo n√£o responde | Verifique se o Ollama est√° ativo e se o modelo foi baixado (`ollama list`). |
| Remo√ß√£o de arquivo falha | Observe o alerta sob o cabe√ßalho de contexto; ele informa o motivo. |
| Porta ocupada | Execute `./run_web.sh --port 9000` (ou ajuste a vari√°vel `WEB_PORT` no `start.sh`). |

---

## üìÑ Licen√ßa

Projeto distribu√≠do nos termos definidos pelo autor original. Consulte o reposit√≥rio para diretrizes de uso e contribui√ß√£o.
