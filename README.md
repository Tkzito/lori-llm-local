# Lori LLM Local

Lori √© uma assistente local para linha de comando e navegador constru√≠da sobre modelos servidos pelo Ollama. O projeto oferece:

- **Backend FastAPI** com WebSocket para streaming de respostas do agente.
- **Interface web reativa** com hist√≥rico, painel de racioc√≠nio, upload de arquivos de contexto e modo claro/escuro.
- **Ferramentas modulares** acess√≠veis via `assistant_cli.tools.*`, reaproveitadas tanto pelo web app quanto pelo CLI.

---

## üÜï Atualiza√ß√µes recentes

- Hist√≥rico agrupado por dia, com t√≠tulo, hor√°rio e pr√©via de cada conversa.
- Handles laterais (‚ò∞ e üß†) reabrem hist√≥rico e painel de racioc√≠nio sem reduzir a √°rea do chat.
- README e menu unificado revisados para refletir o fluxo atual da Web UI.

## üöÄ Primeiros passos

### Pr√©-requisitos

- Python 3.10 ou superior.
- Ollama instalado e executando (padr√£o em `http://localhost:11434`).
- Recomendado: modelos como `mistral` importados no Ollama (`ollama run mistral`).

### Menu unificado

Utilize o menu principal para escolher como interagir com a Lori:

```bash
./start.sh
```

O script cuida de:

1. Iniciar Lori no terminal (CLI)
2. Iniciar Lori no navegador (Web UI)
3. Verificar/iniciar o servi√ßo Ollama
4. Encerrar o Ollama
5. Iniciar tudo (Ollama + Web UI em segundo plano)
6. Encerrar tudo (Web UI + Ollama)
7. Visualizar logs (Ollama, Web UI ou ambos)

O menu mostra o status atual de cada componente e mant√©m a Web UI sempre em `http://127.0.0.1:8001/`. Quando iniciada em segundo plano, os logs ficam em `.lori_web.log` e o PID em `.lori_web.pid`.

A cada a√ß√£o, o script garante que o virtualenv `.venv` exista, instala depend√™ncias de `requirements.txt` quando necess√°rio e verifica se o Ollama responde (perguntando se deve inicializ√°-lo caso esteja parado).

### Ver logs em tempo real

Utilize o script auxiliar `lori-logs.sh` para acompanhar os logs capturados pelo menu:

```bash
# Log do Ollama
./lori-logs.sh ollama

# Log da Web UI em background
./lori-logs.sh web

# Ambos os logs em uma √∫nica sa√≠da
./lori-logs.sh ambos
```

Os arquivos correspondentes s√£o `.lori_ollama.log` e `.lori_web.log`; os PIDs ficam registrados em `.lori_ollama.pid` e `.lori_web.pid`.

### Execu√ß√£o direta (opcional)

Se preferir chamar os modos manualmente, os scripts originais continuam dispon√≠veis:

```bash
./run.sh       # CLI
./run_web.sh   # interface web
```

---

## üñ•Ô∏è Vis√£o geral da interface web

A interface √© dividida em tr√™s √°reas principais:

| Zona | Descri√ß√£o |
| --- | --- |
| **Hist√≥rico** (coluna esquerda) | Conversas agrupadas por dia (Hoje, Ontem, dias da semana, etc.), com t√≠tulo, pr√©via e hor√°rio. Pode ser ocultado pelo bot√£o ‚ò∞ e reaberto pelo handle lateral quando recolhido. |
| **Chat** (centro) | Mostra a conversa com a Lori. Inclui √°rea de anexos, campo de mensagem com envio `Enter`, indicador de digita√ß√£o e o contador de arquivos de contexto. |
| **Racioc√≠nio do agente** (coluna direita) | Exibe pensamentos, chamadas de ferramenta e confirma√ß√µes. Pode ser recolhido pelo bot√£o üß† e reaberto pelo handle com √≠cone de c√©rebro na lateral direita. |

### Arquivos de contexto

- Adicione arquivos pelo bot√£o üìé. Os arquivos s√£o armazenados em `$(LORI_HOME)/uploads` (padr√£o: `/tmp/lori/uploads` em sistemas Unix-like).
- Cada arquivo aparece com nome, tamanho, √≠cone e bot√£o **Remover**. Enquanto o backend processa o pedido o item exibe um spinner.
- O bot√£o **Limpar** remove todos os arquivos carregados. O contador abaixo do t√≠tulo indica quantos arquivos est√£o ativos.

### Outros recursos √∫teis

- Altern√¢ncia de tema claro/escuro pela op√ß√£o ‚óë na barra superior.
- Hist√≥rico e painel do agente lembram o estado (aberto/fechado) entre sess√µes via `localStorage`.
- Handles laterais (‚ò∞ e üß†) facilitam reabrir os pain√©is sem tomar espa√ßo do chat.

### Evolu√ß√µes planejadas

- Automa√ß√£o para reconhecer pedidos de atualiza√ß√£o (ex.: "atualize o pre√ßo do BTC") e refazer consultas relevantes automaticamente.
- Aperfei√ßoar heur√≠sticas para aprender prefer√™ncias por conversa utilizando o hist√≥rico agrupado.

---

## üìÅ Estrutura do projeto

```
assistant-cli/
‚îú‚îÄ‚îÄ assistant_cli/          # N√∫cleo do agente e ferramentas
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Inicializador do m√≥dulo
‚îÇ   ‚îú‚îÄ‚îÄ agent.py            # Loop principal do agente
‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # Entrada do CLI
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Vari√°veis de ambiente e diret√≥rios padr√£o
‚îÇ   ‚îú‚îÄ‚îÄ heuristic_processor.py # Processador de heur√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py    # Cliente para o Ollama
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py       # Testes para o agente
‚îÇ   ‚îú‚îÄ‚îÄ test_tools.py       # Testes para as ferramentas
‚îÇ   ‚îú‚îÄ‚îÄ tools_cli.py        # Ferramentas de linha de comando
‚îÇ   ‚îî‚îÄ‚îÄ tools.py            # Registro de ferramentas/bindings
‚îú‚îÄ‚îÄ web/                    # Backend FastAPI (servi√ßos REST/WebSocket)
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Aplica√ß√£o FastAPI e rotas
‚îÇ   ‚îî‚îÄ‚îÄ static/             # Front-end (index.html, style.css, app.js)
‚îú‚îÄ‚îÄ scripts/                # Scripts de automa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.sh        # Script de inicializa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ run_lori_tests.sh   # Script para rodar testes da Lori
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.sh        # Script para rodar testes
‚îÇ   ‚îú‚îÄ‚îÄ smoke.sh            # Script para testes de fuma√ßa
‚îÇ   ‚îî‚îÄ‚îÄ test_tools.sh       # Script para testar ferramentas
‚îú‚îÄ‚îÄ run.sh                  # Inicializador do CLI
‚îú‚îÄ‚îÄ run_web.sh              # Inicializador da interface web
‚îú‚îÄ‚îÄ start.sh                # Menu unificado com bootstrapping
‚îú‚îÄ‚îÄ lori-logs.sh            # Script para visualizar logs
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îú‚îÄ‚îÄ setup.py                # Script de setup do projeto
‚îî‚îÄ‚îÄ config.ini.template     # Template opcional de configura√ß√£o
```

---

## ‚öôÔ∏è Configura√ß√£o

As principais vari√°veis de ambiente aceitas est√£o em `assistant_cli/config.py`. Algumas relevantes:

| Vari√°vel | Descri√ß√£o | Padr√£o |
| --- | --- | --- |
| `ASSISTANT_MODEL` | Modelo a ser usado no Ollama | `mistral` |
| `OLLAMA_BASE_URL` | Endpoint do Ollama | `http://localhost:11434` |
| `LORI_HOME` | Diret√≥rio base para workspace/cache/uploads | `/tmp/lori` (via `tempfile.gettempdir()`) |
| `ASSISTANT_ROOT` | Raiz permitida para opera√ß√µes de arquivo | `/tmp/lori/workspace` |
| `ASSISTANT_VERBOSE` | Habilita logs de ferramentas no agente | `0` |
| `OLLAMA_USE_GPU` | Define se o Ollama deve usar GPU (`1`) | auto |

Para customizar permanentemente, voc√™ pode criar um `.env` (carregado manualmente) ou exportar as vari√°veis antes de rodar os scripts.

> **Dica GPU**: assegure-se de instalar a vers√£o do Ollama com suporte CUDA, exporte `OLLAMA_USE_GPU=1` (ou configure `~/.ollama/config`) e baixe o modelo desejado (`ollama pull mistral`) antes de iniciar o menu.

---

## üß™ Desenvolvimento e testes

- **Testes:** `pytest`
- **Lint:** `ruff check .`
- Os arquivos do front ficam em `web/static/`. Ap√≥s alterar CSS ou JS basta recarregar a p√°gina; o backend roda com `--reload`.
- Anexos enviados pela interface s√£o salvos em `$(LORI_HOME)/uploads` (por padr√£o, `/tmp/lori/uploads`). Limpe manualmente se necess√°rio.

---

## üõ†Ô∏è Solu√ß√£o de problemas

| Sintoma | Como resolver |
| --- | --- |
| Modelo n√£o responde | Verifique se o Ollama est√° em execu√ß√£o e se o modelo foi baixado (`ollama list`). |
| N√£o consigo remover arquivo de contexto | Confirme se o item aparece com spinner; se a opera√ß√£o falhar o aviso embaixo do cabe√ßalho trar√° o motivo. |
| Portas ocupadas | Ajuste `run_web.sh` passando `--port` para outro valor (`./run_web.sh --port 9000`). |

---

## üìÑ Licen√ßa

Este projeto √© distribu√≠do nos termos definidos pelo autor. Consulte o reposit√≥rio original para mais detalhes sobre uso e contribui√ß√µes.
