# Lori LLM Local

Lori √© uma assistente local para linha de comando e navegador constru√≠da sobre modelos servidos pelo Ollama. O projeto oferece:

- **Backend FastAPI** com WebSocket para streaming de respostas do agente.
- **Interface web reativa** com hist√≥rico, painel de racioc√≠nio, upload de arquivos de contexto e modo claro/escuro.
- **Ferramentas modulares** acess√≠veis via `assistant_cli.tools.*`, reaproveitadas tanto pelo web app quanto pelo CLI.

---

## üöÄ Primeiros passos

### Pr√©-requisitos

- Python 3.10 ou superior.
- Ollama instalado e executando (padr√£o em `http://localhost:11434`).
- Recomendado: modelos como `mistral` importados no Ollama (`ollama run mistral`).

### Menu unificado

Utilize o menu principal para escolher como interagir com a Lori:

```bash
./start.sh
```

O script cuida de:

1. Iniciar Lori no terminal (CLI)
2. Iniciar Lori no navegador (Web UI)
3. Verificar/iniciar o servi√ßo Ollama
4. Encerrar o Ollama
5. Iniciar tudo (Ollama + Web UI em segundo plano)
6. Encerrar tudo (Web UI + Ollama)

O menu mostra o status atual de cada componente e mant√©m a Web UI sempre em `http://127.0.0.1:8001/`. Quando iniciada em segundo plano, os logs ficam em `.lori_web.log` e o PID em `.lori_web.pid`.

A cada a√ß√£o, o script garante que o virtualenv `.venv` exista, instala depend√™ncias de `requirements.txt` quando necess√°rio e verifica se o Ollama responde (perguntando se deve inicializ√°-lo caso esteja parado).

### Ver logs em tempo real

Utilize o script auxiliar `lori-logs.sh` para acompanhar os logs capturados pelo menu:

```bash
# Log do Ollama
./lori-logs.sh ollama

# Log da Web UI em background
./lori-logs.sh web

# Ambos os logs em uma √∫nica sa√≠da
./lori-logs.sh ambos
```

Os arquivos correspondentes s√£o `.lori_ollama.log` e `.lori_web.log`; os PIDs ficam registrados em `.lori_ollama.pid` e `.lori_web.pid`.

### Execu√ß√£o direta (opcional)

Se preferir chamar os modos manualmente, os scripts originais continuam dispon√≠veis:

```bash
./run.sh       # CLI
./run_web.sh   # interface web
```

---

## üñ•Ô∏è Vis√£o geral da interface web

A interface √© dividida em tr√™s √°reas principais:

| Zona | Descri√ß√£o |
| --- | --- |
| **Hist√≥rico** (coluna esquerda) | Lista conversas recentes. Pode ser ocultada/mostrada pelo bot√£o ‚ò∞ na barra superior. |
| **Chat** (centro) | Mostra a conversa com a Lori. Inclui √°rea de anexos, campo de mensagem com envio `Enter`, indicador de digita√ß√£o e o contador de arquivos de contexto. |
| **Racioc√≠nio do agente** (coluna direita) | Exibe pensamentos, chamadas de ferramenta e confirma√ß√µes. Pode ser recolhido pelo bot√£o üß† ou pelo handle flutuante que aparece quando fechado. |

### Arquivos de contexto

- Adicione arquivos pelo bot√£o üìé. Os arquivos s√£o armazenados em `~/lori/uploads` (padr√£o).
- Cada arquivo aparece com nome, tamanho, √≠cone e bot√£o **Remover**. Enquanto o backend processa o pedido o item exibe um spinner.
- O bot√£o **Limpar** remove todos os arquivos carregados. O contador abaixo do t√≠tulo indica quantos arquivos est√£o ativos.

### Outros recursos √∫teis

- Altern√¢ncia de tema claro/escuro pela op√ß√£o ‚óë na barra superior.
- Hist√≥rico e painel do agente lembram o estado (aberto/fechado) entre sess√µes via `localStorage`.

---

## üìÅ Estrutura do projeto

```
assistant-cli/
‚îú‚îÄ‚îÄ assistant_cli/          # N√∫cleo do agente e ferramentas
‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # Entrada do CLI
‚îÇ   ‚îú‚îÄ‚îÄ agent.py            # Loop principal do agente
‚îÇ   ‚îú‚îÄ‚îÄ tools.py            # Registro de ferramentas/bindings
‚îÇ   ‚îî‚îÄ‚îÄ config.py           # Vari√°veis de ambiente e diret√≥rios padr√£o
‚îú‚îÄ‚îÄ web/                    # Backend FastAPI (servi√ßos REST/WebSocket)
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Aplica√ß√£o FastAPI e rotas
‚îÇ   ‚îî‚îÄ‚îÄ static/             # Front-end (index.html, style.css, app.js)
‚îú‚îÄ‚îÄ run.sh                  # Inicializador do CLI
‚îú‚îÄ‚îÄ run_web.sh              # Inicializador da interface web
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îî‚îÄ‚îÄ config.ini.template     # Template opcional de configura√ß√£o
```

---

## ‚öôÔ∏è Configura√ß√£o

As principais vari√°veis de ambiente aceitas est√£o em `assistant_cli/config.py`. Algumas relevantes:

| Vari√°vel | Descri√ß√£o | Padr√£o |
| --- | --- | --- |
| `ASSISTANT_MODEL` | Modelo a ser usado no Ollama | `mistral` |
| `OLLAMA_BASE_URL` | Endpoint do Ollama | `http://localhost:11434` |
| `LORI_HOME` | Diret√≥rio base para workspace/cache/uploads | `~/lori` |
| `ASSISTANT_ROOT` | Raiz permitida para opera√ß√µes de arquivo | `~/lori/workspace` |
| `ASSISTANT_VERBOSE` | Habilita logs de ferramentas no agente | `0` |
| `OLLAMA_USE_GPU` | Define se o Ollama deve usar GPU (`1`) | auto |

Para customizar permanentemente, voc√™ pode criar um `.env` (carregado manualmente) ou exportar as vari√°veis antes de rodar os scripts.

> **Dica GPU**: assegure-se de instalar a vers√£o do Ollama com suporte CUDA, exporte `OLLAMA_USE_GPU=1` (ou configure `~/.ollama/config`) e baixe o modelo desejado (`ollama pull mistral`) antes de iniciar o menu.

---

## üß™ Desenvolvimento e testes

- **Testes:** `pytest`
- **Lint:** `ruff check .`
- Os arquivos do front ficam em `web/static/`. Ap√≥s alterar CSS ou JS basta recarregar a p√°gina; o backend roda com `--reload`.
- Anexos enviados pela interface s√£o salvos em `~/lori/uploads`. Limpe manualmente se necess√°rio.

---

## üõ†Ô∏è Solu√ß√£o de problemas

| Sintoma | Como resolver |
| --- | --- |
| Modelo n√£o responde | Verifique se o Ollama est√° em execu√ß√£o e se o modelo foi baixado (`ollama list`). |
| N√£o consigo remover arquivo de contexto | Confirme se o item aparece com spinner; se a opera√ß√£o falhar o aviso embaixo do cabe√ßalho trar√° o motivo. |
| Portas ocupadas | Ajuste `run_web.sh` passando `--port` para outro valor (`./run_web.sh --port 9000`). |

---

## üìÑ Licen√ßa

Este projeto √© distribu√≠do nos termos definidos pelo autor. Consulte o reposit√≥rio original para mais detalhes sobre uso e contribui√ß√µes.
